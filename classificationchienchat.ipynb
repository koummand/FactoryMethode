{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/koummand/FactoryMethode/blob/master/classificationchienchat.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEDJ3LYXxoe9",
        "outputId": "f8eea849-7b1f-4e15-c99e-ec08151b25ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "!kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "6LIoGmOc1Qrp",
        "outputId": "6672504f-f601-4dfb-c091-bf582ee824c3"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-15f3b8e1-3a2a-4820-b0f7-ee35f4458b6f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-15f3b8e1-3a2a-4820-b0f7-ee35f4458b6f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle(1).json to kaggle(1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle(1).json': b'{\"username\":\"koummandjustin\",\"key\":\"369717a1d0779c39a178ebca8f102889\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm5Yv34m3I8f",
        "outputId": "fcd9919d-78bf-4899-96e6-abe4690a0684"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/kaggle/api/kaggle_api_extended.py\", line 403, in authenticate\n",
            "    raise IOError('Could not find {}. Make sure it\\'s located in'\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ],
      "source": [
        "!mkdir ~/.kaggle\n",
        "!cp kaggle(1).json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle(1).json\n",
        "!kaggle competitions download -c dogs-vs-cats"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -qq train.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdFgSjgzkXWa",
        "outputId": "ac01b33e-f014-4855-cba4-9e8fa81b3af3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unzip:  cannot find or open train.zip, train.zip.zip or train.zip.ZIP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "sWWUhBiS_3ux",
        "outputId": "6e0352ec-d2d5-4b38-f232-f60b76038e64"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-ffa793c8c399>\"\u001b[0;36m, line \u001b[0;32m71\u001b[0m\n\u001b[0;31m    from tensorflow.keras.utils\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Listing 8.6 Copie d'images dans les répertoires d'entraînement, de validation et de test\n",
        "import os, shutil, pathlib\n",
        "# Chemin d'accès au répertoire où l'ensemble de données d'origine a été décompressé\n",
        "original_dir= pathlib.Path(\"train\")\n",
        "# Répertoire où nous allons stocker notre plus petit ensemble de données\n",
        "new_base_dir= pathlib.Path(\"cat_vs_dog_small\")\n",
        "\n",
        "# Fonction utilitaire pour copier les images de chat (et de chien) de l'index\n",
        "    # start_index à l'index end_index dans le sous­répertoire new_base_dir/\n",
        "    # {subset_name}/cat (et /dog). Le \"subset_name\" sera soit \"train\",\n",
        "    # \"validation\", ou \"test\".\n",
        "def make_subset(subset_name, start_index, end_index):\n",
        "      for category in (\"cat\", \"dog\"):\n",
        "        dir = new_base_dir / subset_name / category\n",
        "        os.makedirs(dir)\n",
        "        fnames = [f\"{category}.{i}.jpg\"\n",
        "            for i in range(start_index, end_index)]\n",
        "        for fname in fnames:\n",
        "          shutil.copyfile(src=original_dir / fname, dst=dir / fname)\n",
        "\n",
        "          #  Créez le sous­ensemble\n",
        "      #   de formation avec le premier\n",
        "      #   1 000 images de\n",
        "      #   chaque catégorie.\n",
        "      make_subset(\"train\", start_index=0, end_index=1000)\n",
        "      # creer les sous ensembles de\n",
        "      # validation avec les 500\n",
        "      # prochaines images de\n",
        "      # chaque catégorie.\n",
        "      make_subset(\"validation\", start_index=1000, end_index=1500)\n",
        "      # Créez le sous­ensemble de\n",
        "      # test avec les 1 000 images\n",
        "      # suivantes de chaque catégorie.\n",
        "      make_subset(\"test\", start_index=1500, end_index=2500)\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Le modèle attend Images RVB de taille 180 × 180\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "\n",
        "      # redimensionez  entrées dans la plage [0, 1] en les divisant par 255\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "      # Regardons comment les dimensions des cartes d'entités changent avec chaque successif couche:\n",
        "model.summary()\n",
        "\n",
        "       # Listing 8.8 Configuration du modèle pour l'entraînement\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        " # Listing 8.9 Utiliser image_dataset_from_directory pour lire des images\n",
        "            # 1 Lisez les fichiers image.\n",
        "            # 2 Décodez le contenu JPEG en grilles RVB de pixels.\n",
        "            # 3 Convertissez­les en tenseurs à virgule flottante.\n",
        "            # 4 Redimensionnez­les à une taille partagée (nous utiliserons 180 × 180).\n",
        "            # 5 Regroupez­les par lots (nous utiliserons des lots de 32 images).\n",
        "from tensorflow.keras.utils\n",
        "import image_dataset_from_directory\n",
        "train_dataset = image_dataset_from_directory(new_base_dir / \"train\", image_size=(180, 180), batch_size=32)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory( new_base_dir / \"validation\", image_size=(180, 180), batch_size=32)\n",
        "\n",
        "test_dataset = image_dataset_from_directory( new_base_dir / \"test\", image_size=(180, 180), batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Nous allons considérer 1 000 échantillons, où chaque échantillon est un vecteur de taille 16 :\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "random_numbers = np.random.normal(size=(1000, 16))\n",
        "      # La méthode de classe from_tensor_slices() peut être\n",
        "      # importer numpy en tant que\n",
        "      # np importer tensorflow en tant que tf\n",
        "      # utilisé pour créer un Dataset à partir d'un tableau NumPy,\n",
        "      # ou un tuple ou un dict de tableaux NumPy\n",
        "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
        "        #  Au début, notre ensemble de données ne produit que des échantillons uniques :\n",
        "for i, element in enumerate(dataset):\n",
        " print(element.shape)\n",
        " if i >= 2:\n",
        "  break\n",
        "\n",
        "\n",
        "        #  Nous pouvons utiliser la méthode .batch() pour regrouper les données :\n",
        "batched_dataset = dataset.batch(32)\n",
        "for i, element in enumerate(batched_dataset):\n",
        " print(element.shape)\n",
        " if i >= 2:\n",
        "  break\n",
        "\n",
        "# Plus largement, nous avons accès à une gamme de méthodes d'ensemble de données utiles, telles que\n",
        "\n",
        "        #  Mélange les éléments dans un tampon\n",
        "# .shuffle(buffer_size)\n",
        "        # pré­extrait un tampon d'éléments dans la mémoire GPU pour obtenir une meilleure utilisation de l'appareil.\n",
        "# .prefetch(buffer_size)\n",
        "        #  Applique une transformation arbitraire à chaque élément du ensemble de données\n",
        "        #  la fonction appelable, qui s'attend à prendre en entrée un seul élément produit par l'ensemble de données).\n",
        "# .map(callable)\n",
        "\n",
        "        # La méthode .map() , en particulier, est celle que vous utiliserez souvent. Voici un exemple.\n",
        "        # Nous l'utiliserons pour remodeler les éléments de notre ensemble de données de jouets de la forme (16,) à la\n",
        "        # forme (4, 4) :\n",
        "\n",
        "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
        "for i, element in enumerate(reshaped_dataset):\n",
        "  print(element.shape)\n",
        "  if i >= 2:\n",
        "   break\n",
        "\n",
        "\n",
        "\n",
        "        # Listing 8.10 Affichage des formes des données et des étiquettes générées par le jeu de données\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "  print(\"data batch shape:\", data_batch.shape)\n",
        "  print(\"labels batch shape:\", labels_batch.shape)\n",
        "  break\n",
        "\n",
        "          # data batch shape: (32, 180, 180, 3)\n",
        "          # labels batch shape: (32,)\n",
        "\n",
        "          # ModelCheckpoint enregistrement du model apres chaque iteration\n",
        "          # save_best_only=True et monitor=\"val_loss\": enregistrement du model si et seulement si la perte de validation diminue\n",
        "\n",
        "         # Listing 8.11 Ajustement du modèle à l'aide d'un jeu de données\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"convnet_from_scratch.keras\",\n",
        "        save_best_only=True,\n",
        "        monitor=\"val_loss\")]\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "            epochs=30,\n",
        "            validation_data=validation_dataset,\n",
        "            callbacks=callbacks)\n",
        "\n",
        "        # Listing 8.12 Affichage des courbes de perte et de précision pendant l'entraînement\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy = history.history[\"accuracy\"]\n",
        "val_accuracy = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "        # Ces parcelles sont caractéristiques du surajustement. La précision de la formation augmente de manière linéaire au fil du\n",
        "        # temps, jusqu'à atteindre près de 100 %, tandis que la précision de la validation culmine à 75 %\n",
        "\n",
        "        # Listing 8.13 Évaluation du modèle sur le jeu de test\n",
        "\n",
        "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "        # 8.2.5 Utilisation de l'augmentation des données\n",
        "        # Listing 8.14 Définir une étape d'augmentation de données à ajouter à un modèle d'image\n",
        "\n",
        "data_augmentation = keras.Sequential([\n",
        "            # applique un retournement horizontal à 50 % des images qui le traversent\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "            # fait pivoter les images d'entrée d'une valeur aléatoire dans la plage [–10 %, +10 %] (ce sont fractions d'un cercle complet ­\n",
        "            # en degrés, la plage serait [–36 degrés, +36 degrés])\n",
        " layers.RandomRotation(0.1),\n",
        "            # effectue un zoom avant ou arrière sur l'image selon un facteur aléatoire plage [­20%, +20%]\n",
        " layers.RandomZoom(0.2),])\n",
        "\n",
        "        # Listing 8.15 Affichage d'images d'apprentissage augmentées de manière aléatoire\n",
        "plt.figure(figsize=(10, 10))\n",
        "        # Nous pouvons utiliser take(N) pour n'échantillonner que N lots de l'ensemble de données. C'est\n",
        "        # revient à insérer une rupture dans la\n",
        "for images, _ in train_dataset.take(1):\n",
        "  for i in range(9):\n",
        "            # Appliquer le augmentation scène à la lot de images.\n",
        "    augmented_images = data_augmentation(images)\n",
        "    ax= plt.subplot(3, 3, i + 1)\n",
        "            # Affichez la première image du lot de sortie. Pour chacune des neuf itérations, il s'agit d'un\n",
        "            # différentes augmentations de la même image\n",
        "    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "          # Listing 8.16 Définition d'un nouveau convnet qui inclut l'augmentation et l'abandon d'image\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(x)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "          optimizer=\"rmsprop\",\n",
        "          metrics=[\"accuracy\"])\n",
        "\n",
        "          # Listing 8.17 Entraînement du convnet régularisé\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
        "            save_best_only=True,\n",
        "            monitor=\"val_loss\")]\n",
        "\n",
        "history = model.fit( train_dataset,\n",
        "                  epochs=100,\n",
        "                  validation_data=validation_dataset,\n",
        "                  callbacks=callbacks)\n",
        "\n",
        "        #  La précision de la validation se retrouve constamment dans la plage de 80 à 85 %, une grande\n",
        "\n",
        "\n",
        "        # Listing 8.18 Évaluation du modèle sur le jeu de test\n",
        "test_model = keras.models.load_model(\"convnet_from_scratch_with_augmentation.keras\")\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "      # Listing 8.19 Instanciation de la base convolutive VGG16\n",
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "              # weights spécifie le point de contrôle de poids à partir duquel initialiser le modèle.\n",
        "weights=\"imagenet\",\n",
        "              # include_top fait référence à l'inclusion (ou non) du classificateur densément connecté sur\n",
        "              # sommet du réseau. Par défaut, ce classificateur densément connexe correspond à\n",
        "              # les 1 000 cours d'ImageNet. Parce que nous avons l'intention d'utiliser notre propre densément\n",
        "              # classificateur connecté (avec seulement deux classes : chat et chien), nous n'avons pas besoin de\n",
        "              # l'inclure\n",
        "include_top=False,\n",
        "              # input_shape est la forme des tenseurs d'image que nous allons alimenter au réseau.\n",
        "input_shape=(180, 180, 3))\n",
        "\n",
        "conv_base.summary()\n",
        "\n",
        "          # augmentation des données.\n",
        "import numpy as np\n",
        "def get_features_and_labels(dataset):\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for images, labels in dataset:\n",
        "    preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "    features = conv_base.predict(preprocessed_images)\n",
        "    all_features.append(features)\n",
        "    all_labels.append(labels)\n",
        "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
        "\n",
        "train_features, train_labels = get_features_and_labels(train_dataset)\n",
        "\n",
        "val_features, val_labels = get_features_and_labels(validation_dataset)\n",
        "\n",
        "test_features, test_labels = get_features_and_labels(test_dataset)\n",
        "\n",
        "\n",
        "train_features.shape\n",
        "\n",
        "\n",
        "           #  Listing 8.21 Définition et entraînement du classifieur densément connecté\n",
        "inputs = keras.Input(shape=(5, 5, 512))\n",
        "                # Notez l'utilisation de la couche\n",
        "                # Flatten avant de passer les\n",
        "                # entités à une couche Dense.\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "              optimizer=\"rmsprop\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\n",
        "            filepath=\"feature_extraction.keras\",\n",
        "            save_best_only=True,\n",
        "            monitor=\"val_loss\")]\n",
        "\n",
        "history = model.fit(train_features, train_labels,\n",
        "              epochs=20,\n",
        "              validation_data=(val_features, val_labels),\n",
        "              callbacks=callbacks)\n",
        "\n",
        "                # L'apprentissage est très rapide car nous n'avons affaire qu'à deux couches Dense : une époque prend\n",
        "                # moins d'une seconde même sur CPU.\n",
        "\n",
        "                # Regardons les courbes de perte et de précision pendant l'entraînement (voir figure 8.13).\n",
        "\n",
        "          #  Listing 8.22 Tracé des résultats\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
        "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
        "plt.title(\"Training and validation accuracy\")\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
        "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
        "plt.title(\"Training and validation loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " #EXTRACTION DE CARACTÉRISTIQUES ET AUGMENTATION DE DONNÉES\n",
        "\n",
        "            # Dans Keras, nous figeons(gelons) une couche ou un modèle en définissant son attribut entraînable sur False.\n",
        "\n",
        "      # Listing 8.23 Instanciation et gel de la base convolutive VGG16\n",
        "conv_base = keras.applications.vgg16.VGG16(\n",
        "              weights=\"imagenet\",\n",
        "              include_top=False)\n",
        "\n",
        "                        # La définition de trainable sur False vide la liste des pondérations entraînables de la couche ou du modèle.\n",
        "conv_base.trainable = False\n",
        "\n",
        "      # Listing 8.24 Impression de la liste des poids entraînables avant et après congélation\n",
        "                         # Il s'agit du nombre de poids pouvant être entraînés avant de geler la base de conversion : 26\n",
        "conv_base.trainable = True\n",
        "print(\"Ceci est le nombre de poids entraînables \"\n",
        "      \"avant de geler la base de conversion :\",\n",
        "      len(conv_base.trainable_weights))\n",
        "\n",
        "                          # Il s'agit du nombre de poids pouvant être entraînés après avoir gelé la base de conversion : 0\n",
        "conv_base.trainable = False\n",
        "print(\"Ceci est le nombre de poids pouvant être entraînés \"\n",
        "      \"après avoir gelé la base de conversion :\",\n",
        "      len(conv_base.trainable_weights))\n",
        "\n",
        "\n",
        "\n",
        "        # Listing 8.25 Ajout d'une étape d'augmentation de données et d'un classifieur à la base convolutive\n",
        "data_augmentation = keras.Sequential(\n",
        "                           [layers.RandomFlip(\"horizontal\"),\n",
        "                            layers.RandomRotation(0.1),\n",
        "                            layers.RandomZoom(0.2),\n",
        "                          ])\n",
        "\n",
        "inputs = keras.Input(shape=(180, 180, 3))\n",
        "                        #  Appliquer l'augmentation des données.\n",
        "x = data_augmentation(inputs)\n",
        "x = keras.applications.vgg16.preprocess_input(x)\n",
        "                        # Appliquer la mise à l'échelle de la valeur d'entrée.\n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "          optimizer=\"rmsprop\",\n",
        "          metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")]\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "            epochs=50,\n",
        "            validation_data=validation_dataset,\n",
        "            callbacks=callbacks)\n",
        "\n",
        "                #  Comme vous pouvez le constater, nous atteignons une précision\n",
        "                #  de validation de plus de 98 %. C'est une nette amélioration par rapport au modèle précédent\n",
        "\n",
        "\n",
        "                                      # Vérifions la précision du test.\n",
        "\n",
        "        # Listing 8.26 Évaluation du modèle sur le jeu de test\n",
        "\n",
        "test_model = keras.models.load_model(\"feature_extraction_with_data_augmentation.keras\")\n",
        "\n",
        "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 8.3.2 Affiner un modèle pré­entraîné (Une autre technique largement utilisée pour la réutilisation du modèle,)\n",
        "\n",
        "\n",
        "          # Listing 8.27 Geler tous les calques jusqu'au quatrième à partir du dernier\n",
        "conv_base.trainable = True\n",
        "for layer in conv_base.layers[:-4]:\n",
        " layer.trainable = False\n",
        "\n",
        "\n",
        "\n",
        "          # Listing 8.28 Affiner le modèle\n",
        "model.compile(loss=\"binary_crossentropy\",\n",
        "      optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "      metrics=[\"accuracy\"])\n",
        "\n",
        "callbacks = [keras.callbacks.ModelCheckpoint(\n",
        "      filepath=\"fine_tuning.keras\",\n",
        "      save_best_only=True,\n",
        "      monitor=\"val_loss\")]\n",
        "\n",
        "history = model.fit(train_dataset,\n",
        "              epochs=30,\n",
        "              validation_data=validation_dataset,\n",
        "              callbacks=callbacks)\n",
        "\n",
        "                # On peut enfin évaluer ce modèle sur les données de test :\n",
        "model = keras.models.load_model(\"fine_tuning.keras\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "print(f\"Test accuracy: {test_acc:.3f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RoktSC9t63UU"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZ28SjipcI5TG3F05xFwH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}